{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  French Trot Horse Racing Prediction Using Artificial Neural Networks\n",
        "\n",
        "**CIS*6020: Artificial Intelligence**\n",
        "\n",
        "University of Guelph\n",
        "\n",
        "Submitted By: Santosh Kumar Satapathy\n",
        "Under the Guidance of: Dr. Neil Bruce\n",
        "\n"
      ],
      "metadata": {
        "id": "lCjiBzXpKrrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "8Yj3u71lVESB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import Google Drive library to import data from Google Drive as mentioned in the ReadMe Instructions\n",
        "from google.colab import drive\n",
        "\n",
        "# Sklearn Libraries Used\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Tensorflow Libraries Used\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "CM5o2NtsU_uz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive and Read Data"
      ],
      "metadata": {
        "id": "f9kZuIO6WJUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Reminder: The dataset parquet file is supposed to be located at My Drive path in Google Drive\n",
        "# Else, the path below needs to be updated before running:\n",
        "\n",
        "data = pd.read_parquet('/content/drive/My Drive/trots_2013-2022.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7LYdvMBWPBW",
        "outputId": "8c0188e8-4cb0-4f60-8e5e-a1b0c0e43557"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data PreProcessing and Data Engineering"
      ],
      "metadata": {
        "id": "eNwk7ICUWe1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finish Position and Finish Position 2"
      ],
      "metadata": {
        "id": "1t7lLX34YYKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8tOTnnC79NOW"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary space characters\n",
        "data['FinishPosition'] = data['FinishPosition'].str.replace(' ','')\n",
        "\n",
        "# Feature Engineering: New Feature that captures whether a horse was able to finish the race or got disqualified\n",
        "data['FinishPosition2'] = np.where(data['FinishPosition'].isin(['BS','UN','PU','DQ','FL','NP','UR','WC']),1,0)\n",
        "\n",
        "# In the original feature, assign the last finish position in case of disqualified horses\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='BS',19,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='UN',20,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='PU',21,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='DQ',22,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='FL',23,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='NP',24,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='UR',25,data['FinishPosition'])\n",
        "data['FinishPosition'] = np.where(data['FinishPosition']=='WC',26,data['FinishPosition'])\n",
        "\n",
        "# Convert to Numerical Feature\n",
        "data['FinishPosition'] = data['FinishPosition'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beaten Margin"
      ],
      "metadata": {
        "id": "DTVL4La6YfT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In case of Beaten Margin of disqualified horses, we are changing the margin from default 999 value to the max value of beaten margin of that race + the standard deviation\n",
        "calc = data[data['BeatenMargin']!=999][['BeatenMargin','RaceID']].groupby(by=['RaceID'],as_index=False).max().join(data[data['BeatenMargin']!=999][['BeatenMargin','RaceID']].groupby(by=['RaceID'],as_index=False).std(),rsuffix='2')\n",
        "del calc['RaceID2']\n",
        "data = data.merge(calc,how='left',on=['RaceID'])\n",
        "del calc\n",
        "data['BeatenMargin2'] = data['BeatenMargin2'].fillna(0)\n",
        "\n",
        "# Assign new value and clean dataset\n",
        "data['BeatenMargin_x'] = np.where(data['BeatenMargin_x']==999,data['BeatenMargin_y']+data['BeatenMargin2'],data['BeatenMargin_x'])\n",
        "del data['BeatenMargin2']\n",
        "del data['BeatenMargin_y']\n",
        "data['BeatenMargin'] = data['BeatenMargin_x']\n",
        "del data['BeatenMargin_x']"
      ],
      "metadata": {
        "id": "T9E56oimYgwY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputing NULL Values"
      ],
      "metadata": {
        "id": "PAwkbGgIYssu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This loop checks for Null OR Garbage Values in the dataset and prints these features and values\n",
        "for i in data.columns:\n",
        "    if ((len(data[(data[i].isna())|(data[i].isnull())|(data[i]==None)|(data[i]=='None')|(data[i]=='')].index)/len(data.index)*100)>0):\n",
        "        print(i+\" Null or Garbage Value Count :  \"+str(len(data[(data[i].isna())|(data[i].isnull())|(data[i]==None)|(data[i]=='None')|(data[i]=='')].index)))\n",
        "        data[i] = np.where((data[i].isna())|(data[i].isnull())|(data[i]==None)|(data[i]=='None')|(data[i]==''),'NA',data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMkIkFgxZCgR",
        "outputId": "c6cb525e-4b87-47e7-a65e-c96550bce69c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgeRestriction Null or Garbage Value Count :  633\n",
            "ClassRestriction Null or Garbage Value Count :  29588\n",
            "HandicapType Null or Garbage Value Count :  1042792\n",
            "SexRestriction Null or Garbage Value Count :  888663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Features"
      ],
      "metadata": {
        "id": "NVKVaYMxZG_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping the data sorted by Race Time as we know that we will be feeding the dataset in a chronological order while training so it would be easy to feed later\n",
        "data.sort_values(by=['RaceStartTime','FinishPosition'],ascending=[True,False])\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# Defining our Race Data Related Features\n",
        "data_col = ['AgeRestriction','Barrier','ClassRestriction','CourseIndicator','DamID','Distance','FoalingCountry','FoalingDate','FrontShoes','GoingAbbrev',\n",
        "            'GoingID','HandicapDistance','HandicapType','HindShoes','HorseAge','HorseID','JockeyID','RaceGroup','RacePrizemoney','RaceStartTime','RacingSubType',\n",
        "            'Saddlecloth','SireID','StartType','StartingLine','Surface','TrackID','TrainerID','WeightCarried','WetnessScale','RaceID','SexRestriction','Gender']\n",
        "\n",
        "# Defining our Performance Data Related Features. We will be using Beaten Margin from all these features as our Label.\n",
        "perf_col = ['BeatenMargin','FinishPosition','FinishPosition2','Disqualified', 'PIRPosition', 'Prizemoney', 'RaceOverallTime', 'PriceSP', 'NoFrontCover', 'PositionInRunning',\n",
        "               'WideOffRail']\n",
        "\n",
        "# Alter the names of Performance Features for easy recognition\n",
        "for i in data[perf_col].columns:\n",
        "  data[\"PERF_\"+i] = data[i]\n",
        "  del data[i]\n",
        "\n",
        "perf_col = ['PERF_BeatenMargin','PERF_FinishPosition','PERF_FinishPosition2','PERF_Disqualified', 'PERF_PIRPosition', 'PERF_Prizemoney', 'PERF_RaceOverallTime', 'PERF_PriceSP', 'PERF_NoFrontCover', 'PERF_PositionInRunning',\n",
        "            'PERF_WideOffRail']\n",
        "\n",
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9TIOnpjQZNzl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Restriction: Type and Prize Money Limit"
      ],
      "metadata": {
        "id": "5dwSRL0pZan6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning this feature to create two new features, class type and the prize limit to enter the race\n",
        "data['ClassRestriction_Type'] = data['ClassRestriction'].str.split(' ',expand=True)[0].str.split('$',expand=True)[0]\n",
        "data['ClassRestriction_Type'] = np.where(((data['ClassRestriction_Type'].isna())|(data['ClassRestriction_Type']=='')),data['ClassRestriction'].str.split(' ',expand=True)[1].str.split('$',expand=True)[0],data['ClassRestriction_Type'])\n",
        "data['ClassRestriction_Type'] = np.where(((data['ClassRestriction_Type'].isna())|(data['ClassRestriction_Type']=='')),data['ClassRestriction'].str.split(' ',expand=True)[2],data['ClassRestriction_Type'])\n",
        "\n",
        "data['ClassRestriction_Price'] = data['ClassRestriction'].str.split(' ',expand=True)[0].str.split('$',expand=True)[1]\n",
        "data['ClassRestriction_Price'] = np.where((data['ClassRestriction_Price'].isna())|(data['ClassRestriction_Price']==''),data['ClassRestriction'].str.split(' ',expand=True)[1].str.split('$',expand=True)[1],data['ClassRestriction_Price'])\n",
        "data['ClassRestriction_Price'] = np.where((data['ClassRestriction_Price'].isna())|(data['ClassRestriction_Price']==''),0,data['ClassRestriction_Price'])\n",
        "\n",
        "del data['ClassRestriction']\n",
        "del data['GoingID']"
      ],
      "metadata": {
        "id": "hMPxrhuBZx9h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoding"
      ],
      "metadata": {
        "id": "kc-opfy-Zz6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Gender'] = np.where(data['Gender']=='F',0,1)\n",
        "\n",
        "data['RacingSubType'] = np.where(data['RacingSubType']=='TM',1,0)\n",
        "\n",
        "data['StartType'] = np.where(data['StartType']=='V',1,0)\n",
        "\n",
        "data['PERF_Disqualified'] = np.where(data['PERF_Disqualified']==False,0,1)"
      ],
      "metadata": {
        "id": "sjnStyIWZ8QF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Foaling Date"
      ],
      "metadata": {
        "id": "LFOV0JvcaIVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['FoalingDate'] = pd.to_datetime(data['FoalingDate'])\n",
        "data['RaceStartTime'] = pd.to_datetime(data['RaceStartTime'])\n",
        "\n",
        "# calculating the age of the horse at the time of the race in days\n",
        "data['FoalingDate'] = (data['RaceStartTime']-data['FoalingDate']).dt.days"
      ],
      "metadata": {
        "id": "79qteBlbaI0x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding"
      ],
      "metadata": {
        "id": "AJmd7ubTaNM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Categorical_Features = ['AgeRestriction','ClassRestriction_Type','CourseIndicator','FoalingCountry','GoingAbbrev','HandicapType','RaceGroup','Surface','SexRestriction']\n",
        "\n",
        "Numerical_Features = ['Barrier','ClassRestriction_Price','Distance','FoalingDate','FrontShoes','HandicapDistance','HindShoes','HorseAge','RacePrizemoney','Saddlecloth','StartType',\n",
        "                      'StartingLine','WeightCarried','WetnessScale','PERF_BeatenMargin','PERF_FinishPosition','PERF_FinishPosition2','PERF_Disqualified','PERF_PIRPosition','PERF_Prizemoney','PERF_RaceOverallTime','PERF_PriceSP','PERF_NoFrontCover',\n",
        "                      'PERF_PositionInRunning','PERF_WideOffRail','StartType','RacingSubType']\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "for i in Categorical_Features:\n",
        "    # Applying fit_transform, and saving the arrays in a dataframe\n",
        "    a = pd.DataFrame(enc.fit_transform(data[[i]]).toarray())\n",
        "    # Mapping each column's names\n",
        "    a.columns = enc.get_feature_names_out()\n",
        "    #Joining to original dataframe\n",
        "    data = data.join(a)\n",
        "    #Removing original feature\n",
        "    del data[i]\n",
        "\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "for i in Numerical_Features:\n",
        "  data[i] = data[i].astype(float)\n",
        "\n",
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VqAuRWuJZ_Xc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering past performance metrics of Dam, Jockey, Trainer, Sire"
      ],
      "metadata": {
        "id": "np_ziXFPabi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating number of races in each horse\n",
        "data = data.merge(data.groupby(by=['RaceID'],as_index=False).count().rename(columns={'Barrier':'RaceHorseCount'})[['RaceID','RaceHorseCount']],how='left',on=['RaceID'])\n",
        "\n",
        "# Calculating Dam's Total Children and Childern's Total Races\n",
        "data = data.merge(data.groupby(by=['DamID'],as_index=False).agg({ \"HorseID\": pd.Series.nunique, \"Barrier\": pd.Series.count}).rename(columns={'HorseID':'Dam_TotalChild','Barrier':'Dam_TotalChildRaces'}),how='left',on=['DamID'])\n",
        "\n",
        "# Calculating Jockey's Total Races, Total Horses Riden and Mean Beaten Margin\n",
        "data = data.merge(data.groupby(by=['JockeyID'],as_index=False).agg({\"RaceID\": pd.Series.nunique,\"HorseID\": pd.Series.nunique,\"PERF_BeatenMargin\": pd.Series.mean}).rename(columns={'RaceID':'Jockey_TotalRaces','HorseID':'Jockey_TotalHorses','PERF_BeatenMargin':'Jockey_MeanMargin'}),how='left',on=['JockeyID'])\n",
        "\n",
        "# Calculating Trainer's Total Races, Total Horses Trained and Mean Beaten Margin\n",
        "data = data.merge(data.groupby(by=['TrainerID'],as_index=False).agg({\"RaceID\": pd.Series.nunique,\"HorseID\": pd.Series.nunique,\"PERF_BeatenMargin\": pd.Series.mean}).rename(columns={'RaceID':'Trainer_TotalRaces','HorseID':'Trainer_TotalHorses','PERF_BeatenMargin':'Trainer_MeanMargin'}),how='left',on=['TrainerID'])\n",
        "\n",
        "# Calculating Sire's Total Children and Childern's Total Races\n",
        "data = data.merge(data.groupby(by=['SireID'],as_index=False).agg({ \"HorseID\": pd.Series.nunique, \"Barrier\": pd.Series.count}).rename(columns={'HorseID':'Sire_TotalChild','Barrier':'Sire_TotalChildRaces'}),how='left',on=['SireID'])"
      ],
      "metadata": {
        "id": "adM55Ks1bNpU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PAST PERFORMANCE FEATURES"
      ],
      "metadata": {
        "id": "zl6qRwRZbQTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sort_values(by=['HorseID','RaceStartTime'],ascending=[True,True])\n",
        "data['HorseID'] = data['HorseID'].astype(str)\n",
        "\n",
        "# Adding the data of last 3 race's performing metrics by shifting the dataset\n",
        "data = data.join(data[[i for i in data.columns if i in ['HorseID','PERF_FinishPosition2','RaceStartTime','PERF_BeatenMargin','PERF_Disqualified','PERF_PIRPosition','PERF_RaceOverallTime','PERF_NoFrontCover','PERF_PositionInRunning','PERF_WideOffRail','Distance','HandicapDistance','Surface','WeightCarried','WetnessScale']]].shift(1), rsuffix='_Past1')\n",
        "data = data.join(data[[i for i in data.columns if i in ['HorseID','PERF_FinishPosition2','RaceStartTime','PERF_BeatenMargin','PERF_Disqualified','PERF_PIRPosition','PERF_RaceOverallTime','PERF_NoFrontCover','PERF_PositionInRunning','PERF_WideOffRail','Distance','HandicapDistance','Surface','WeightCarried','WetnessScale']]].shift(2), rsuffix='_Past2')\n",
        "data = data.join(data[[i for i in data.columns if i in ['HorseID','PERF_FinishPosition2','RaceStartTime','PERF_BeatenMargin','PERF_Disqualified','PERF_PIRPosition','PERF_RaceOverallTime','PERF_NoFrontCover','PERF_PositionInRunning','PERF_WideOffRail','Distance','HandicapDistance','Surface','WeightCarried','WetnessScale']]].shift(3), rsuffix='_Past3')\n",
        "\n",
        "# Also calculating the time since the horse's last 3 races\n",
        "data['RaceStartTime_Past1'] = (data['RaceStartTime']-data['RaceStartTime_Past1']).dt.days\n",
        "data['RaceStartTime_Past2'] = (data['RaceStartTime']-data['RaceStartTime_Past2']).dt.days\n",
        "data['RaceStartTime_Past3'] = (data['RaceStartTime']-data['RaceStartTime_Past3']).dt.days\n",
        "\n",
        "# ONLY KEEP THE DATA OF HORSES THAT HAVE ATLEAST 4 RACES, as the data used for training needs atleast 3 prior races data\n",
        "data = data[(data['HorseID']==data['HorseID_Past1'])&(data['HorseID']==data['HorseID_Past2'])&(data['HorseID']==data['HorseID_Past3'])]\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "data = data.sort_values(by=['RaceStartTime','PERF_FinishPosition'],ascending=[True,False])\n",
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "kk4enpvXYUgX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Processed Dataset in Pickle format on Google Drive"
      ],
      "metadata": {
        "id": "aXfwxgxRcCj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a sanity check, removes any feature that has only 1 unique value\n",
        "for i in data.columns:\n",
        "  if data[i].nunique() < 2:\n",
        "    del data[i]\n",
        "\n",
        "# Drop Performance Features that we would not need to evaluate the results\n",
        "data = data.drop(columns=['PERF_RaceOverallTime','DamID','JockeyID','SireID','TrackID','TrainerID','HorseID_Past1','HorseID_Past2','HorseID_Past3','PERF_Disqualified','PERF_PIRPosition','PERF_Prizemoney','PERF_RaceOverallTime','PERF_PriceSP','PERF_NoFrontCover','PERF_PositionInRunning','PERF_WideOffRail'])\n",
        "\n",
        "# Save the Data in Pickle format on your drive\n",
        "# data.to_pickle('/content/drive/My Drive/FrenchTrotProcessedData.pkl')\n",
        "\n",
        "# Unmount your drive\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "zI489BAFcDah"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling the Dataset"
      ],
      "metadata": {
        "id": "-ke-BLs3cltZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OM1rYGHCS6rV"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')\n",
        "# data = pd.read_pickle('/content/drive/My Drive/FrenchTrotProcessedData.pkl')\n",
        "del data['RaceStartTime']\n",
        "del data['PERF_FinishPosition']\n",
        "# drive.flush_and_unmount()\n",
        "\n",
        "\n",
        "# # Use Standard Scaler to scale our features\n",
        "# data_toscale = data.drop(columns=['HorseID','RaceID','PERF_BeatenMargin'])\n",
        "#\n",
        "# # Fit the features\n",
        "# scaler = StandardScaler().fit(data_toscale)\n",
        "# # Transform the features based on the fit\n",
        "# data_toscale = scaler.transform(data_toscale)\n",
        "# # Join Labels back\n",
        "# data = data[['RaceID','HorseID','PERF_BeatenMargin']].join(pd.DataFrame(data_toscale,columns=data.drop(columns=['RaceID','HorseID','PERF_BeatenMargin']).columns))\n",
        "# del data_toscale\n",
        "\n",
        "\n",
        "# EITHER USE STANDARD SCALER OR NORMALIZER AND COMMENT THE OTHER SECTION\n",
        "\n",
        "# Use the Normalizer from Keras library\n",
        "normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "\n",
        "# Train the normalizer on Features\n",
        "normalizer.adapt(data.drop(columns=['PERF_BeatenMargin','HorseID','RaceID','PERF_FinishPosition2'], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHOOSE A RACE TO PROCEED\n",
        "\n",
        "This Race ID will be used to predict the model's output for this race. Any Race ID present in the dataset could be fed here. It is recommended to select a race from a later date in the dataset as it would have more past race samples to train on. Also, if the participating horses have more races in the past, the more data the model would have to train on.\n",
        "\n",
        "Some samples are provided that can be used to run the notebook:\n",
        "1642204\n",
        "1638090\n",
        "1648771\n",
        "1656357\n",
        "1650165"
      ],
      "metadata": {
        "id": "fM-EknzbdBhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raceid = 1638090\n",
        "\n",
        "# Only store the data of all the horses and their races that participated in this race and free up your memory!\n",
        "\n",
        "data = data[data['HorseID'].isin(data[data['RaceID']==raceid]['HorseID'])]\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# Split into different dataframes for each horse\n",
        "data_split = [y for x, y in data.groupby('HorseID')]"
      ],
      "metadata": {
        "id": "E_Pq5-hVdBPe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL GRID SEARCH TO FIND THE BEST NETWORK PARAMETERS\n",
        "\n",
        "NOTE: The below grid search code is commented as it takes significant amount of time to run, the cells below directly run the model on the best parameters and the Output of this Grid Search is attached in Excel Format with this Project."
      ],
      "metadata": {
        "id": "LHEMigaRfqBb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H2KjqiUg9MEe"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')\n",
        "\n",
        "# neurons_layer1 = [10,25,50,100]\n",
        "# neurons_layer2 = [0,10,25,50,100]\n",
        "# neurons_layer3 = [0,10,25,50,100]\n",
        "# epochs = [20,50,100]\n",
        "\n",
        "# cont = 0\n",
        "# for e in tqdm(epochs):\n",
        "#   for n1 in neurons_layer1:\n",
        "#     for n2 in neurons_layer2:\n",
        "#       for n3 in neurons_layer3:\n",
        "\n",
        "#         if (e == 20) and (n1 == 10) and (n2 == 0) and (n3 == 0):\n",
        "#           cont = 1\n",
        "\n",
        "#         elif cont == 1:\n",
        "\n",
        "#           # Functional model using pre-processing layer\n",
        "#           inputs = tf.keras.Input(shape=151)\n",
        "#           x = normalizer(inputs)\n",
        "\n",
        "#           # Input Layer\n",
        "#           x = tf.keras.layers.Dense(151,activation='relu') (x)\n",
        "\n",
        "#           # Hidden Layer 1\n",
        "#           x = tf.keras.layers.Dense(n1,activation='relu') (x)\n",
        "\n",
        "#           if n2 != 0:\n",
        "#             # Hidden Layer 2\n",
        "#             x = tf.keras.layers.Dense(n2,activation='relu') (x)\n",
        "#             if n3 != 0:\n",
        "#               # Hidden Layer 3\n",
        "#               x = tf.keras.layers.Dense(n3,activation='relu') (x)\n",
        "\n",
        "#           # Output Layer\n",
        "#           output = tf.keras.layers.Dense(1) (x)\n",
        "\n",
        "#           model = tf.keras.Model(inputs,output)\n",
        "#           # model.summary()\n",
        "\n",
        "#           # Train seperate models for each horse\n",
        "#           for split_frame in data_split:\n",
        "#             horseid = split_frame['HorseID'].unique()\n",
        "\n",
        "#             # Do not train on races that the horse got disqualified in\n",
        "#             split_frame = split_frame[(split_frame['RaceID']<raceid)&(split_frame['PERF_FinishPosition2']!=1)]\n",
        "\n",
        "#             X = split_frame.drop(columns=['PERF_BeatenMargin','HorseID','RaceID','PERF_FinishPosition2'], axis=1)\n",
        "#             y = split_frame[['PERF_BeatenMargin']]\n",
        "\n",
        "#             model = tf.keras.Model(inputs,output)\n",
        "\n",
        "#             # Complie the model and optimise on MSE\n",
        "#             model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "\n",
        "#             # Fit the training data sample by sample by Batch Size of 1, do not shuffle, as we have sorted the data in chronological order to preserve the effect of recent races while training the model\n",
        "#             model.fit(X,y,batch_size=1,shuffle=False, epochs=e,verbose=0)\n",
        "\n",
        "#           data = data[data['RaceID']==raceid]\n",
        "\n",
        "\n",
        "#           # Once all the models are trained for each horse in the chosen race, we predict their beaten margin\n",
        "#           final = []\n",
        "#           for i in data['HorseID'].unique():\n",
        "#             y_pred = model.predict(data[data['HorseID']==i].drop(columns=['PERF_BeatenMargin','HorseID','RaceID','PERF_FinishPosition2'], axis=1),verbose=0)\n",
        "\n",
        "#             store = pd.DataFrame(y_pred).join(data[data['HorseID']==i]['PERF_BeatenMargin'].reset_index(drop=True))\n",
        "#             final.append(store)\n",
        "#           final = pd.concat(final)\n",
        "\n",
        "#           final = final.sort_values(by=['PERF_BeatenMargin'],ascending=True)\n",
        "#           final = final.reset_index(drop=True)\n",
        "#           final = final.sort_values(by=[0],ascending=True)\n",
        "#           final = final.reset_index(drop=False)\n",
        "\n",
        "#           # Print the Epoch, Neurons in Layer 1, 2, 3, Mean deviation in predictes finish position of Top 3 horses and RMSE in Beaten Margin for all horses\n",
        "#           print(e,n1,n2,n3,\"RANK: \",str((final[final['index']<3]['index']-final[final['index']<3].index).mean()),\"RMSE: \",np.sqrt(((final[0]-final['PERF_BeatenMargin'])**2).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRID SEARCH OUTPUT\n",
        "\n",
        "Given that the above Grid Search would take a lot of time to run and find out the best network parameters that best minimise the RMSE, we can run the cell below that would directly use the best parameters and predict the output.\n",
        "\n",
        "NOTE: THE OUTPUT OF ABOVE LOOP IS ATTACHED WITH THIS PROJECT IN EXCEL FORMAT"
      ],
      "metadata": {
        "id": "8sEtz0XMhJpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Predictions"
      ],
      "metadata": {
        "id": "8eHiO9xWhiME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zrm2WpmjGA7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45137a49-3c4e-46e1-bfa0-68a3292daee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 151)]             0         \n",
            "                                                                 \n",
            " normalization (Normalizatio  (None, 151)              303       \n",
            " n)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 151)               22952     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1520      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,006\n",
            "Trainable params: 24,703\n",
            "Non-trainable params: 303\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [01:26<00:00,  4.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d483a666a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 126ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d483a665ab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "RANK:  1 RMSE:  12.531485796809166\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "#Functional model using pre-processing layer\n",
        "inputs = tf.keras.Input(shape=151)\n",
        "x = normalizer(inputs)\n",
        "x = tf.keras.layers.Dense(151,activation='relu') (x)\n",
        "x = tf.keras.layers.Dense(10,activation='relu') (x)\n",
        "x = tf.keras.layers.Dense(10,activation='relu') (x)\n",
        "x = tf.keras.layers.Dense(10,activation='relu') (x)\n",
        "output = tf.keras.layers.Dense(1) (x)\n",
        "model = tf.keras.Model(inputs,output)\n",
        "model.summary()\n",
        "\n",
        "for split_frame in tqdm(data_split):\n",
        "  horseid = split_frame['HorseID'].unique()\n",
        "\n",
        "  split_frame = split_frame[(split_frame['RaceID']<raceid)&(split_frame['PERF_FinishPosition2']!=1)]\n",
        "\n",
        "  X = split_frame.drop(columns=['PERF_BeatenMargin','HorseID','RaceID','PERF_FinishPosition2'], axis=1)\n",
        "  y = split_frame[['PERF_BeatenMargin']]\n",
        "\n",
        "  model = tf.keras.Model(inputs,output)\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "  model.fit(X,y,batch_size=1,shuffle=False, epochs=20,verbose=0)\n",
        "  model.save_weights('/content/drive/My Drive/model_weight_'+str(horseid[0])+'.h5')\n",
        "  del model\n",
        "\n",
        "\n",
        "data = data[data['RaceID']==raceid]\n",
        "\n",
        "\n",
        "final = []\n",
        "for i in data['HorseID'].unique():\n",
        "  model = tf.keras.Model(inputs,output)\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "  model.load_weights('/content/drive/My Drive/model_weight_'+i+'.h5')\n",
        "\n",
        "  y_pred = model.predict(data[data['HorseID']==i].drop(columns=['PERF_BeatenMargin','HorseID','RaceID','PERF_FinishPosition2'], axis=1))\n",
        "\n",
        "  store = pd.DataFrame(y_pred).join(data[data['HorseID']==i]['PERF_BeatenMargin'].reset_index(drop=True))\n",
        "  final.append(store)\n",
        "final = pd.concat(final)\n",
        "\n",
        "final = final.sort_values(by=['PERF_BeatenMargin'],ascending=True)\n",
        "final = final.reset_index(drop=True)\n",
        "final = final.sort_values(by=[0],ascending=True)\n",
        "final = final.reset_index(drop=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL PREDICTIONS ON YOUR CHOSEN RACE:"
      ],
      "metadata": {
        "id": "gs33GzNpjWiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GcTo6jWTwDs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64cea85-3c87-4d16-f93c-43d99a0bf646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTED RANK of the Horse the Finished First using the Model that we just trained above:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"PREDICTED RANK of the Horse the Finished First using the Model that we just trained above: \",str(final[final['index']==0].index[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "b5Y6AedRwOXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88af6a96-ec60-4da2-e345-8e5efdf37cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE in Predicting All Horse's Beaten Margin:  12.53 m\n"
          ]
        }
      ],
      "source": [
        "print(\"RMSE in Predicting All Horse's Beaten Margin: \",round(np.sqrt(((final[0]-final['PERF_BeatenMargin'])**2).mean()),2),\"m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: You can go back to the Race ID selection section, select a different Race ID and the re-run the whole notebook, the code would train the model on the dataset for that race and predict the Output."
      ],
      "metadata": {
        "id": "5Z7LIROImBi9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2GB8jjAmBNp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 3955319,
          "sourceId": 6884483,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30558,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}